{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb7f5766-e804-4af3-a801-24916c133663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e2c654-5989-4929-a371-5acec8003de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e0468ec-e9cc-4374-bab9-d7313e06824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../../data/All.csv'\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "524fe792-8385-4d11-8191-2bff64b1571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = df['SessionId'].sample(n=1000, random_state=random_seed)\n",
    "df = df[df['SessionId'].isin(sampled_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da6ef367-4055-4bd1-a205-2a4d47953ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU4REC(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, final_act='tanh',\n",
    "                 dropout_hidden=.5, dropout_input=0, batch_size=50, embedding_dim=-1, use_cuda=False):\n",
    "        super(GRU4REC, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_hidden = dropout_hidden\n",
    "        self.dropout_input = dropout_input\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.use_cuda = use_cuda\n",
    "        self.device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "        self.onehot_buffer = self.init_emb()\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.create_final_activation(final_act)\n",
    "        if self.embedding_dim != -1:\n",
    "            self.look_up = nn.Embedding(input_size, self.embedding_dim)\n",
    "            self.gru = nn.GRU(self.embedding_dim, self.hidden_size, self.num_layers, dropout=self.dropout_hidden)\n",
    "        else:\n",
    "            self.gru = nn.GRU(self.input_size, self.hidden_size, self.num_layers, dropout=self.dropout_hidden)\n",
    "        self = self.to(self.device)\n",
    "\n",
    "    def create_final_activation(self, final_act):\n",
    "        if final_act == 'tanh':\n",
    "            self.final_activation = nn.Tanh()\n",
    "        elif final_act == 'relu':\n",
    "            self.final_activation = nn.ReLU()\n",
    "        elif final_act == 'softmax':\n",
    "            self.final_activation = nn.Softmax()\n",
    "        elif final_act == 'softmax_logit':\n",
    "            self.final_activation = nn.LogSoftmax()\n",
    "        elif final_act.startswith('elu-'):\n",
    "            self.final_activation = nn.ELU(alpha=float(final_act.split('-')[1]))\n",
    "        elif final_act.startswith('leaky-'):\n",
    "            self.final_activation = nn.LeakyReLU(negative_slope=float(final_act.split('-')[1]))\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "\n",
    "        if self.embedding_dim == -1:\n",
    "            embedded = self.onehot_encode(input)\n",
    "            if self.training and self.dropout_input > 0: embedded = self.embedding_dropout(embedded)\n",
    "            embedded = embedded.unsqueeze(0)\n",
    "        else:\n",
    "            embedded = input.unsqueeze(0)\n",
    "            embedded = self.look_up(embedded)\n",
    "\n",
    "        output, hidden = self.gru(embedded, hidden) #(num_layer, B, H)\n",
    "        output = output.view(-1, output.size(-1))  #(B,H)\n",
    "        logit = self.final_activation(self.h2o(output))\n",
    "\n",
    "        return logit, hidden\n",
    "\n",
    "    def init_emb(self):\n",
    "        onehot_buffer = torch.FloatTensor(self.batch_size, self.output_size)\n",
    "        onehot_buffer = onehot_buffer.to(self.device)\n",
    "        return onehot_buffer\n",
    "\n",
    "    def onehot_encode(self, input):\n",
    "        self.onehot_buffer.zero_()\n",
    "        index = input.view(-1, 1)\n",
    "        one_hot = self.onehot_buffer.scatter_(1, index, 1)\n",
    "        return one_hot\n",
    "\n",
    "    def embedding_dropout(self, input):\n",
    "        p_drop = torch.Tensor(input.size(0), 1).fill_(1 - self.dropout_input)\n",
    "        mask = torch.bernoulli(p_drop).expand_as(input) / (1 - self.dropout_input)\n",
    "        mask = mask.to(self.device)\n",
    "        input = input * mask\n",
    "        return input\n",
    "\n",
    "    def init_hidden(self):\n",
    "        try:\n",
    "            h0 = torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.device)\n",
    "        except:\n",
    "            self.device = 'cpu'\n",
    "            h0 = torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.device)\n",
    "        return h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9e2573f-ee55-4f67-86a1-5440e3027376",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, df, sep=',', session_key='SessionId', item_key='ItemId', time_key='Time', n_sample=-1, itemmap=None, itemstamp=None, time_sort=False):\n",
    "        # Read csv\n",
    "        self.df = df\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "        self.time_sort = time_sort\n",
    "        if n_sample > 0:\n",
    "            self.df = self.df[:n_sample]\n",
    "\n",
    "        self.add_item_indices(itemmap=itemmap)\n",
    "        self.df.sort_values([session_key, time_key], inplace=True)\n",
    "        self.click_offsets = self.get_click_offset()\n",
    "        self.session_idx_arr = self.order_session_idx()\n",
    "\n",
    "    def add_item_indices(self, itemmap=None):\n",
    "        if itemmap is None:\n",
    "            item_ids = self.df[self.item_key].unique()\n",
    "            item2idx = pd.Series(data=np.arange(len(item_ids)),\n",
    "                                 index=item_ids)\n",
    "            itemmap = pd.DataFrame({self.item_key: item_ids,\n",
    "                                   'item_idx': item2idx[item_ids].values})\n",
    "        self.itemmap = itemmap\n",
    "        self.df = pd.merge(self.df, self.itemmap, on=self.item_key, how='inner')\n",
    "\n",
    "    def get_click_offset(self):\n",
    "        offsets = np.zeros(self.df[self.session_key].nunique() + 1, dtype=np.int32)\n",
    "        offsets[1:] = self.df.groupby(self.session_key).size().cumsum()\n",
    "        return offsets\n",
    "\n",
    "    def order_session_idx(self):\n",
    "        if self.time_sort:\n",
    "            sessions_start_time = self.df.groupby(self.session_key)[self.time_key].min().values\n",
    "            session_idx_arr = np.argsort(sessions_start_time)\n",
    "        else:\n",
    "            session_idx_arr = np.arange(self.df[self.session_key].nunique())\n",
    "        return session_idx_arr\n",
    "\n",
    "    @property\n",
    "    def items(self):\n",
    "        return self.itemmap[self.item_key].unique()\n",
    "\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, dataset, batch_size=50, isTrain=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.isTrain = isTrain\n",
    "        \n",
    "        self.total_items = len(set(self.dataset.df['item_idx']))\n",
    "        item_counts = dataset.df['item_idx'].value_counts()\n",
    "        self.item_probabilities = item_counts / item_counts.sum()\n",
    "\n",
    "    def __iter__(self):\n",
    "        df = self.dataset.df\n",
    "        click_offsets = self.dataset.click_offsets\n",
    "        session_idx_arr = self.dataset.session_idx_arr\n",
    "\n",
    "        iters = np.arange(self.batch_size)\n",
    "        maxiter = iters.max()\n",
    "        start = click_offsets[session_idx_arr[iters]]\n",
    "        end = click_offsets[session_idx_arr[iters] + 1]\n",
    "        mask = []\n",
    "        finished = False\n",
    "        batch_items = set()\n",
    "\n",
    "        while not finished:\n",
    "            minlen = (end - start).min()\n",
    "            idx_target = df.item_idx.values[start]\n",
    "\n",
    "            for i in range(minlen - 1):\n",
    "                idx_input = idx_target\n",
    "                idx_target = df.item_idx.values[start + i + 1]\n",
    "                input = torch.LongTensor(idx_input)\n",
    "                target = torch.LongTensor(idx_target)\n",
    "                \n",
    "                batch_items.update(idx_input.tolist())\n",
    "                batch_items.update(idx_target.tolist())\n",
    "\n",
    "                if self.isTrain:\n",
    "                    idx_negative_samples = self.generate_negative_samples(batch_items, num_samples=10)\n",
    "                    negative_samples = torch.LongTensor(idx_negative_samples)\n",
    "                    yield input, target, negative_samples, mask\n",
    "                    \n",
    "                else:\n",
    "                    yield input, target, mask\n",
    "\n",
    "            start = start + (minlen - 1)\n",
    "            mask = np.arange(len(iters))[(end - start) <= 1]\n",
    "            for idx in mask:\n",
    "                maxiter += 1\n",
    "                if maxiter >= len(click_offsets) - 1:\n",
    "                    finished = True\n",
    "                    break\n",
    "                iters[idx] = maxiter\n",
    "                start[idx] = click_offsets[session_idx_arr[maxiter]]\n",
    "                end[idx] = click_offsets[session_idx_arr[maxiter] + 1]\n",
    "\n",
    "    def generate_negative_samples(self, batch_items, num_samples):\n",
    "        try:\n",
    "            all_items = set(range(self.total_items))\n",
    "            available_items = list(all_items - batch_items)\n",
    "            if not available_items:\n",
    "                available_items = list(all_items)\n",
    "            weights = self.item_probabilities[available_items].values\n",
    "            idx_negative_samples = random.choices(available_items, weights=weights, k=num_samples)\n",
    "            \n",
    "            return idx_negative_samples\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(\"An error occurred:\", e)\n",
    "            print(\"all_items:\", all_items)\n",
    "            print(\"batch_items:\", batch_items)\n",
    "            print(\"available_items:\", available_items)\n",
    "            print(\"weights:\", weights)\n",
    "            traceback.print_exc()\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20456215-712b-40f3-8d9a-619b0a334bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, params, optimizer_type='Adagrad', lr=.05,\n",
    "                 momentum=0, weight_decay=0, eps=1e-6):\n",
    "        if optimizer_type == 'RMSProp':\n",
    "            self.optimizer = optim.RMSprop(params, lr=lr, eps=eps, weight_decay=weight_decay, momentum=momentum)\n",
    "        elif optimizer_type == 'Adagrad':\n",
    "            self.optimizer = optim.Adagrad(params, lr=lr, weight_decay=weight_decay)\n",
    "        elif optimizer_type == 'Adadelta':\n",
    "            self.optimizer = optim.Adadelta(params, lr=lr, eps=eps, weight_decay=weight_decay)\n",
    "        elif optimizer_type == 'Adam':\n",
    "            self.optimizer = optim.Adam(params, lr=lr, eps=eps, weight_decay=weight_decay)\n",
    "        elif optimizer_type == 'SparseAdam':\n",
    "            self.optimizer = optim.SparseAdam(params, lr=lr, eps=eps)\n",
    "        elif optimizer_type == 'SGD':\n",
    "            self.optimizer = optim.SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa7b3bb8-5e4d-4a93-9b75-03f6c97f38f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model, train_data, eval_data, optim, use_cuda, loss_func, batch_size, k):\n",
    "        self.model = model\n",
    "        self.train_data = train_data\n",
    "        self.eval_data = eval_data\n",
    "        self.optim = optim\n",
    "        self.loss_func = LossFunctionNeg()\n",
    "        self.evaluation = Evaluation(self.model, self.loss_func, use_cuda, k = k)\n",
    "        self.device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train(self, start_epoch, end_epoch, start_time=None):\n",
    "        if start_time is None:\n",
    "            self.start_time = time.time()\n",
    "        else:\n",
    "            self.start_time = start_time\n",
    "\n",
    "        for epoch in range(start_epoch, end_epoch + 1):\n",
    "            st = time.time()\n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            ndcg = self.evaluation.eval(self.eval_data, self.batch_size)\n",
    "\n",
    "        return ndcg\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "    \n",
    "        def reset_hidden(hidden, mask):\n",
    "            if len(mask) != 0:\n",
    "                hidden[:, mask, :] = 0\n",
    "            return hidden\n",
    "    \n",
    "        hidden = self.model.init_hidden()\n",
    "        dataloader = DataLoader(self.train_data, self.batch_size)\n",
    "        \n",
    "        for ii, (input, target, negative_samples, mask) in tqdm(enumerate(dataloader), total=len(dataloader.dataset.df) // dataloader.batch_size, miniters = 1000):\n",
    "            input = input.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "            negative_samples = negative_samples.to(self.device)\n",
    "    \n",
    "            self.optim.zero_grad()\n",
    "            hidden = reset_hidden(hidden, mask).detach()\n",
    "            logit, hidden = self.model(input, hidden)\n",
    "    \n",
    "            pos_logits = logit[:, target.view(-1)]\n",
    "            neg_logits = logit[:, negative_samples.view(-1)]\n",
    "    \n",
    "            loss = self.loss_func(pos_logits, neg_logits)\n",
    "            losses.append(loss.item())\n",
    "    \n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "    \n",
    "        mean_losses = np.mean(losses)\n",
    "        return mean_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a97fea9d-3650-4a8d-9e4b-89a3d72f5367",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation(object):\n",
    "    def __init__(self, model, loss_func, use_cuda, k=100):\n",
    "        self.model = model\n",
    "        self.loss_func = LossFunction()\n",
    "        self.topk = k\n",
    "        self.device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "    def eval(self, eval_data, batch_size):\n",
    "        self.model.eval()\n",
    "        ndcgs = []\n",
    "        dataloader = DataLoader(eval_data, batch_size, isTrain=False)\n",
    "        with torch.no_grad():\n",
    "            hidden = self.model.init_hidden()\n",
    "            for ii, (input, target, mask) in tqdm(enumerate(dataloader), total=len(dataloader.dataset.df) // dataloader.batch_size, miniters = 1000):\n",
    "\n",
    "                input = input.to(self.device)\n",
    "                target = target.to(self.device)\n",
    "                logit, hidden = self.model(input, hidden)\n",
    "\n",
    "                ndcg = evaluate_ndcg(logit, target, k=self.topk)\n",
    "                ndcgs.append(ndcg)\n",
    "                \n",
    "        mean_ndcg = np.mean(ndcgs)\n",
    "\n",
    "        return mean_ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f08ef165-a7f4-4248-bc16-9b15d13dfdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ndcg(logits, targets, k):\n",
    "    probs = torch.softmax(logits, dim=1).cpu().detach().numpy()\n",
    "\n",
    "    targets_one_hot = np.zeros_like(probs)\n",
    "    targets_one_hot[np.arange(len(targets)), targets.cpu().numpy()] = 1\n",
    "\n",
    "    ndcg = ndcg_score(targets_one_hot, probs, k=k)\n",
    "\n",
    "    return ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44fb7729-ce52-4a18-a64c-4e9e1acbe32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, loss_type='BPR'):\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.loss_type = loss_type\n",
    "        if loss_type == 'TOP1':\n",
    "            self._loss_fn = TOP1Loss()\n",
    "        elif loss_type == 'BPR':\n",
    "            self._loss_fn = BPRLoss()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, logit):\n",
    "        return self._loss_fn(logit)\n",
    "\n",
    "class BPRLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPRLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logit):\n",
    "        diff = logit.diag().view(-1, 1).expand_as(logit) - logit\n",
    "        # final loss\n",
    "        loss = -torch.mean(F.logsigmoid(diff))\n",
    "        return loss\n",
    "\n",
    "class TOP1Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TOP1Loss, self).__init__()\n",
    "    def forward(self, logit):\n",
    "        diff = -(logit.diag().view(-1, 1).expand_as(logit) - logit)\n",
    "        loss = torch.sigmoid(diff).mean() + torch.sigmoid(logit ** 2).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "461fb5f7-0f54-4bbf-9e19-0fd48a4abdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunctionNeg(nn.Module):\n",
    "    def __init__(self, loss_type='BPR'):\n",
    "        super(LossFunctionNeg, self).__init__()\n",
    "        self.loss_type = loss_type\n",
    "        if loss_type == 'TOP1':\n",
    "            self._loss_fn = TOP1LossNeg()\n",
    "        elif loss_type == 'BPR':\n",
    "            self._loss_fn = BPRLossNeg()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, pos_logits, neg_logits):\n",
    "        return self._loss_fn(pos_logits, neg_logits)\n",
    "\n",
    "class BPRLossNeg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPRLossNeg, self).__init__()\n",
    "\n",
    "    def forward(self, pos_logits, neg_logits):\n",
    "        \n",
    "        pos_logits_expanded = pos_logits.unsqueeze(2)\n",
    "        neg_logits_expanded = neg_logits.unsqueeze(1)\n",
    "\n",
    "        diff = pos_logits_expanded - neg_logits_expanded\n",
    "        loss = -torch.mean(F.logsigmoid(diff))\n",
    "\n",
    "        return loss\n",
    "\n",
    "class TOP1LossNeg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TOP1LossNeg, self).__init__()\n",
    "    def forward(self, logit):\n",
    "        diff = -(logit.diag().view(-1, 1).expand_as(logit) - logit)\n",
    "        loss = torch.sigmoid(diff).mean() + torch.sigmoid(logit ** 2).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bef5dc2-decb-4b83-8645-7a5349c1c564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall(indices, targets):\n",
    "    targets = targets.view(-1, 1).expand_as(indices)\n",
    "    hits = (targets == indices).nonzero()\n",
    "    if len(hits) == 0:\n",
    "        return 0\n",
    "    n_hits = (targets == indices).nonzero()[:, :-1].size(0)\n",
    "    recall = float(n_hits) / targets.size(0)\n",
    "    return recall\n",
    "\n",
    "\n",
    "def get_mrr(indices, targets):\n",
    "    tmp = targets.view(-1, 1)\n",
    "    targets = tmp.expand_as(indices)\n",
    "    hits = (targets == indices).nonzero()\n",
    "    ranks = hits[:, -1] + 1\n",
    "    ranks = ranks.float()\n",
    "    rranks = torch.reciprocal(ranks)\n",
    "    mrr = torch.sum(rranks).data / targets.size(0)\n",
    "    return mrr\n",
    "\n",
    "\n",
    "def evaluate(indices, targets, k=20):\n",
    "    _, indices = torch.topk(indices, k, -1)\n",
    "    recall = get_recall(indices, targets)\n",
    "    mrr = get_mrr(indices, targets)\n",
    "    return recall, mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "790c4f1d-488f-4964-9a10-35c3ea59119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8449bcf2-efa7-47eb-bb12-c375fe729899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(model, sigma):\n",
    "    if sigma is not None:\n",
    "        for p in model.parameters():\n",
    "            if sigma != -1 and args.sigma != -2:\n",
    "                sigma = sigma\n",
    "                p.data.uniform_(-sigma, sigma)\n",
    "            elif len(list(p.size())) > 1:\n",
    "                sigma = np.sqrt(6.0 / (p.size(0) + p.size(1)))\n",
    "                if args.sigma == -1:\n",
    "                    p.data.uniform_(-sigma, sigma)\n",
    "                else:\n",
    "                    p.data.uniform_(0, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01b8c7e1-6d38-4c87-a684-c471654909cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = df['ItemId'].nunique()\n",
    "hidden_size = 100\n",
    "output_size = input_size\n",
    "final_act = 'tanh'\n",
    "num_layers = 3\n",
    "use_cuda = torch.cuda.is_available()\n",
    "batch_size = 50\n",
    "dropout_input = 0\n",
    "dropout_hidden = 0.5\n",
    "embedding_dim = -1\n",
    "\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "optimizer_type = 'Adagrad'\n",
    "lr = 0.01\n",
    "weight_decay = 0\n",
    "momentum = 0\n",
    "eps = 1e-6\n",
    "\n",
    "loss_type = 'BPR'\n",
    "loss_function = LossFunction(loss_type=loss_type)\n",
    "k = 100\n",
    "\n",
    "n_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb5cf980-9eda-4d3c-9a5e-d70c3107960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcgs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ec2a8bc-94b5-4e02-8d6f-25271c41e389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████████████████████████████████████▉      | 3090/3350 [02:02<00:10, 25.15it/s]\n",
      " 74%|███████████████████████████████████████████████████████████▎                    | 621/837 [01:41<00:35,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████████████████████████████████████▌      | 3071/3350 [02:13<00:12, 23.03it/s]\n",
      " 70%|████████████████████████████████████████████████████████▍                       | 590/837 [01:40<00:42,  5.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|████████████████████████████████████████████████████████████████████████      | 3095/3350 [05:41<00:28,  9.07it/s]\n",
      " 76%|█████████████████████████████████████████████████████████████                   | 639/837 [01:48<00:33,  5.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████████████████████████████████████▌      | 3072/3350 [02:11<00:11, 23.33it/s]\n",
      " 69%|███████████████████████████████████████████████████████▏                        | 578/837 [01:38<00:44,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████████████████████████████████████▉      | 3090/3350 [02:12<00:11, 23.26it/s]\n",
      " 69%|███████████████████████████████████████████████████████▌                        | 581/837 [01:42<00:44,  5.69it/s]\n"
     ]
    }
   ],
   "source": [
    "group_kfold = GroupKFold(n_splits=5)\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(group_kfold.split(df, groups=df['SessionId'])):\n",
    "    print(f\"Starting fold {fold + 1}\")\n",
    "\n",
    "    train_data_df = df.iloc[train_index]\n",
    "    valid_data_df = df.iloc[valid_index]\n",
    "    train_data = Dataset(train_data_df)\n",
    "    valid_data = Dataset(valid_data_df)\n",
    "    \n",
    "    model = GRU4REC(input_size = input_size, hidden_size = hidden_size, output_size = output_size, final_act=final_act, num_layers=num_layers, use_cuda=use_cuda, batch_size=batch_size, dropout_input=dropout_input, dropout_hidden=dropout_hidden, embedding_dim=embedding_dim)\n",
    "    init_model(model, sigma)\n",
    "    optimizer = Optimizer(model.parameters(), optimizer_type=optimizer_type, lr=lr, weight_decay=weight_decay, momentum=momentum, eps=eps)\n",
    "    trainer = Trainer(model, train_data=train_data, eval_data=valid_data, optim=optimizer, use_cuda=use_cuda, loss_func=loss_function, batch_size=batch_size, k=k)\n",
    "    ndcg = trainer.train(0, n_epochs - 1)\n",
    "    \n",
    "    ndcgs.append(ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8efb7e8-74c2-47b9-bcb3-a4bf9583dfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NDCG@100: 0.021980521933475616\n"
     ]
    }
   ],
   "source": [
    "average_ndcg = np.mean(ndcgs)\n",
    "print(\"Average NDCG@100:\", average_ndcg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
